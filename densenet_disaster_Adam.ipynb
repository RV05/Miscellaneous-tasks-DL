{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "densenet disaster_Adam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RV05/Miscellaneous-tasks-DL/blob/main/densenet_disaster_Adam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01lAEMug9uHN",
        "outputId": "d28c584b-d7e6-44af-ca80-578e4eb548dd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsYuPMHtVa8p",
        "outputId": "451a2539-6632-4f94-a513-e9abf1327417"
      },
      "source": [
        "cd /content/drive/MyDrive/deep learning"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/deep learning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT5oQ9GQegzo"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "# import the necessary packages\n",
        "import config\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.applications import EfficientNetB7\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications import VGG19\n",
        "\n",
        "\n",
        "from tensorflow.keras.applications import ResNet101\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
        "# \thelp=\"path to output loss/accuracy plot\")\n",
        "# args = vars(ap.parse_args())\n",
        "# # determine the total number of image paths in training, validation,\n",
        "# # and testing directories\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2W2oRSbfwYi"
      },
      "source": [
        "totalTrain = len(list(paths.list_images(config.TRAIN_PATH)))\n",
        "totalVal = len(list(paths.list_images(config.VAL_PATH)))\n",
        "totalTest = len(list(paths.list_images(config.TEST_PATH)))\n",
        "trainAug = ImageDataGenerator(\n",
        "\t# rotation_range=25,\n",
        "\t# zoom_range=0.1,\n",
        "\t# width_shift_range=0.1,\n",
        "\t# height_shift_range=0.1,\n",
        "\t# shear_range=0.2,\n",
        "\t# horizontal_flip=True,\n",
        "  # vertical_flip=True,\n",
        "\t# fill_mode=\"nearest\"\n",
        "  )"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWJ8ZBoSf8hg"
      },
      "source": [
        "# initialize the validation/testing data augmentation object (which\n",
        "# we'll be adding mean subtraction to)\n",
        "valAug = ImageDataGenerator()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lIkzyUDgAfo"
      },
      "source": [
        "# define the ImageNet mean subtraction (in RGB order) and set the\n",
        "# the mean subtraction value for each of the data augmentation\n",
        "# objects\n",
        "# mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
        "# trainAug.mean = mean\n",
        "# valAug.mean = mean"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph4f5GXagD-7",
        "outputId": "57b0265d-1f9f-4341-be5a-c5f1ea1bfb02"
      },
      "source": [
        "trainGen = trainAug.flow_from_directory(\n",
        "\tconfig.TRAIN_PATH,\n",
        "\tclass_mode=\"categorical\",\n",
        "\ttarget_size=(200, 200),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=True,\n",
        "\tbatch_size=config.BS)\n",
        "# initialize the validation generator\n",
        "valGen = valAug.flow_from_directory(\n",
        "\tconfig.VAL_PATH,\n",
        "\tclass_mode=\"categorical\",\n",
        "\ttarget_size=(200, 200),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=False,\n",
        "\tbatch_size=config.BS)\n",
        "# initialize the testing generator\n",
        "testGen = valAug.flow_from_directory(\n",
        "\tconfig.TEST_PATH,\n",
        "\tclass_mode=\"categorical\",\n",
        "\ttarget_size=(200, 200),\n",
        "\tcolor_mode=\"rgb\",\n",
        "\tshuffle=False,\n",
        "\tbatch_size=config.BS)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3309 images belonging to 6 classes.\n",
            "Found 31 images belonging to 6 classes.\n",
            "Found 1022 images belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4po5oj4TggC_",
        "outputId": "a2c0ab19-0f8e-4363-f6f6-3ce7914db2e5"
      },
      "source": [
        "print(\"[INFO] preparing model...\")\n",
        "baseModel = DenseNet121(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(200, 200, 3)))\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D()(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "headModel = Dense(256, activation=\"relu\")(headModel)\n",
        "\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(len(config.CLASSES), activation=\"softmax\")(headModel)\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "# compile the model\n",
        "opt = Adam(lr=config.INIT_LR, decay=config.INIT_LR / config.NUM_EPOCHS)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "# train the model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] preparing model...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 0s 0us/step\n",
            "29097984/29084464 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7cBdl5Of3aG",
        "outputId": "86dcc301-d3a3-4918-e112-d1860a35eade"
      },
      "source": [
        "\n",
        "# load the ResNet-50 network, ensuring the head FC layer sets are left\n",
        "# off\n",
        "\n",
        "print(\"[INFO] training model...\")\n",
        "H = model.fit_generator(\n",
        "\ttrainGen,\n",
        "\tsteps_per_epoch=totalTrain // config.BS,\n",
        "\tvalidation_data=valGen,\n",
        "\tvalidation_steps=totalVal // config.BS,\n",
        "\tepochs=config.NUM_EPOCHS)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] training model...\n",
            "Epoch 1/200\n",
            "413/413 [==============================] - 1656s 4s/step - loss: 1.4657 - accuracy: 0.4680 - val_loss: 1.5646 - val_accuracy: 0.3333\n",
            "Epoch 2/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 1.0136 - accuracy: 0.6247 - val_loss: 1.5157 - val_accuracy: 0.4583\n",
            "Epoch 3/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.8175 - accuracy: 0.7037 - val_loss: 1.6161 - val_accuracy: 0.4167\n",
            "Epoch 4/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.6779 - accuracy: 0.7601 - val_loss: 1.8403 - val_accuracy: 0.5000\n",
            "Epoch 5/200\n",
            "413/413 [==============================] - 28s 68ms/step - loss: 0.5634 - accuracy: 0.7964 - val_loss: 1.6789 - val_accuracy: 0.4583\n",
            "Epoch 6/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.4724 - accuracy: 0.8261 - val_loss: 1.7074 - val_accuracy: 0.5000\n",
            "Epoch 7/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.3759 - accuracy: 0.8685 - val_loss: 1.6120 - val_accuracy: 0.5833\n",
            "Epoch 8/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.3239 - accuracy: 0.8858 - val_loss: 2.0830 - val_accuracy: 0.4583\n",
            "Epoch 9/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.2291 - accuracy: 0.9215 - val_loss: 2.2593 - val_accuracy: 0.4583\n",
            "Epoch 10/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.2036 - accuracy: 0.9294 - val_loss: 1.9196 - val_accuracy: 0.5417\n",
            "Epoch 11/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.1779 - accuracy: 0.9382 - val_loss: 1.8458 - val_accuracy: 0.5417\n",
            "Epoch 12/200\n",
            "413/413 [==============================] - 28s 69ms/step - loss: 0.1290 - accuracy: 0.9579 - val_loss: 2.4539 - val_accuracy: 0.5417\n",
            "Epoch 13/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.1543 - accuracy: 0.9479 - val_loss: 2.3745 - val_accuracy: 0.4583\n",
            "Epoch 14/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.0927 - accuracy: 0.9664 - val_loss: 3.6208 - val_accuracy: 0.5000\n",
            "Epoch 15/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.1870 - accuracy: 0.9418 - val_loss: 2.2921 - val_accuracy: 0.5833\n",
            "Epoch 16/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.1089 - accuracy: 0.9615 - val_loss: 2.2553 - val_accuracy: 0.5000\n",
            "Epoch 17/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.0658 - accuracy: 0.9794 - val_loss: 3.8967 - val_accuracy: 0.4583\n",
            "Epoch 18/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0964 - accuracy: 0.9709 - val_loss: 3.8824 - val_accuracy: 0.5417\n",
            "Epoch 19/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.1066 - accuracy: 0.9652 - val_loss: 2.8539 - val_accuracy: 0.5000\n",
            "Epoch 20/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0629 - accuracy: 0.9812 - val_loss: 2.5125 - val_accuracy: 0.5000\n",
            "Epoch 21/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0719 - accuracy: 0.9743 - val_loss: 3.2893 - val_accuracy: 0.5000\n",
            "Epoch 22/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.0504 - accuracy: 0.9797 - val_loss: 3.4700 - val_accuracy: 0.6250\n",
            "Epoch 23/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.1222 - accuracy: 0.9643 - val_loss: 2.7471 - val_accuracy: 0.5000\n",
            "Epoch 24/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.0505 - accuracy: 0.9818 - val_loss: 3.3519 - val_accuracy: 0.5833\n",
            "Epoch 25/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0411 - accuracy: 0.9839 - val_loss: 3.5437 - val_accuracy: 0.4583\n",
            "Epoch 26/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.1170 - accuracy: 0.9661 - val_loss: 2.8085 - val_accuracy: 0.5417\n",
            "Epoch 27/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0388 - accuracy: 0.9870 - val_loss: 4.0598 - val_accuracy: 0.5417\n",
            "Epoch 28/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0536 - accuracy: 0.9806 - val_loss: 3.4683 - val_accuracy: 0.4583\n",
            "Epoch 29/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0287 - accuracy: 0.9870 - val_loss: 4.1309 - val_accuracy: 0.5000\n",
            "Epoch 30/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0878 - accuracy: 0.9712 - val_loss: 2.8131 - val_accuracy: 0.4583\n",
            "Epoch 31/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0347 - accuracy: 0.9891 - val_loss: 3.8241 - val_accuracy: 0.5417\n",
            "Epoch 32/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0928 - accuracy: 0.9706 - val_loss: 3.6152 - val_accuracy: 0.4583\n",
            "Epoch 33/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0695 - accuracy: 0.9773 - val_loss: 2.9394 - val_accuracy: 0.5417\n",
            "Epoch 34/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0326 - accuracy: 0.9876 - val_loss: 3.7329 - val_accuracy: 0.4583\n",
            "Epoch 35/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0239 - accuracy: 0.9891 - val_loss: 3.6676 - val_accuracy: 0.4583\n",
            "Epoch 36/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0609 - accuracy: 0.9806 - val_loss: 3.3028 - val_accuracy: 0.4167\n",
            "Epoch 37/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0813 - accuracy: 0.9724 - val_loss: 4.5863 - val_accuracy: 0.5417\n",
            "Epoch 38/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.0463 - accuracy: 0.9827 - val_loss: 4.4238 - val_accuracy: 0.4167\n",
            "Epoch 39/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0238 - accuracy: 0.9888 - val_loss: 3.6774 - val_accuracy: 0.5417\n",
            "Epoch 40/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.0631 - accuracy: 0.9782 - val_loss: 6.5031 - val_accuracy: 0.3750\n",
            "Epoch 41/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0386 - accuracy: 0.9852 - val_loss: 3.9244 - val_accuracy: 0.5000\n",
            "Epoch 42/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0711 - accuracy: 0.9806 - val_loss: 4.2644 - val_accuracy: 0.4583\n",
            "Epoch 43/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0215 - accuracy: 0.9915 - val_loss: 4.2994 - val_accuracy: 0.5000\n",
            "Epoch 44/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0115 - accuracy: 0.9933 - val_loss: 4.1516 - val_accuracy: 0.4583\n",
            "Epoch 45/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0243 - accuracy: 0.9891 - val_loss: 6.2485 - val_accuracy: 0.4583\n",
            "Epoch 46/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.0868 - accuracy: 0.9749 - val_loss: 3.0938 - val_accuracy: 0.4167\n",
            "Epoch 47/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0634 - accuracy: 0.9776 - val_loss: 3.6149 - val_accuracy: 0.5417\n",
            "Epoch 48/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0110 - accuracy: 0.9942 - val_loss: 3.6185 - val_accuracy: 0.4167\n",
            "Epoch 49/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0110 - accuracy: 0.9939 - val_loss: 4.3101 - val_accuracy: 0.4167\n",
            "Epoch 50/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0109 - accuracy: 0.9921 - val_loss: 4.4790 - val_accuracy: 0.4167\n",
            "Epoch 51/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.0095 - accuracy: 0.9942 - val_loss: 4.6161 - val_accuracy: 0.4583\n",
            "Epoch 52/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0098 - accuracy: 0.9933 - val_loss: 5.2770 - val_accuracy: 0.4167\n",
            "Epoch 53/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0573 - accuracy: 0.9773 - val_loss: 4.2414 - val_accuracy: 0.4583\n",
            "Epoch 54/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.1233 - accuracy: 0.9627 - val_loss: 3.9277 - val_accuracy: 0.5000\n",
            "Epoch 55/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0393 - accuracy: 0.9864 - val_loss: 4.2064 - val_accuracy: 0.5000\n",
            "Epoch 56/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0217 - accuracy: 0.9897 - val_loss: 5.9416 - val_accuracy: 0.4583\n",
            "Epoch 57/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.0271 - accuracy: 0.9885 - val_loss: 5.7723 - val_accuracy: 0.4167\n",
            "Epoch 58/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0650 - accuracy: 0.9770 - val_loss: 4.4940 - val_accuracy: 0.5000\n",
            "Epoch 59/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.0246 - accuracy: 0.9894 - val_loss: 3.8911 - val_accuracy: 0.4583\n",
            "Epoch 60/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0129 - accuracy: 0.9939 - val_loss: 5.8586 - val_accuracy: 0.4167\n",
            "Epoch 61/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0178 - accuracy: 0.9915 - val_loss: 5.2613 - val_accuracy: 0.4583\n",
            "Epoch 62/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.0390 - accuracy: 0.9876 - val_loss: 3.4564 - val_accuracy: 0.5000\n",
            "Epoch 63/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0188 - accuracy: 0.9906 - val_loss: 4.3357 - val_accuracy: 0.4583\n",
            "Epoch 64/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0559 - accuracy: 0.9815 - val_loss: 5.3086 - val_accuracy: 0.4583\n",
            "Epoch 65/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0441 - accuracy: 0.9846 - val_loss: 3.4044 - val_accuracy: 0.4583\n",
            "Epoch 66/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.0190 - accuracy: 0.9903 - val_loss: 4.9234 - val_accuracy: 0.5000\n",
            "Epoch 67/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0088 - accuracy: 0.9936 - val_loss: 5.7234 - val_accuracy: 0.5000\n",
            "Epoch 68/200\n",
            "413/413 [==============================] - 30s 71ms/step - loss: 0.0093 - accuracy: 0.9927 - val_loss: 5.0919 - val_accuracy: 0.5000\n",
            "Epoch 69/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0088 - accuracy: 0.9945 - val_loss: 5.8453 - val_accuracy: 0.4583\n",
            "Epoch 70/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0130 - accuracy: 0.9930 - val_loss: 4.9118 - val_accuracy: 0.4583\n",
            "Epoch 71/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.1073 - accuracy: 0.9688 - val_loss: 5.1108 - val_accuracy: 0.4583\n",
            "Epoch 72/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0589 - accuracy: 0.9839 - val_loss: 3.4801 - val_accuracy: 0.5000\n",
            "Epoch 73/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0463 - accuracy: 0.9852 - val_loss: 4.8350 - val_accuracy: 0.4167\n",
            "Epoch 74/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0103 - accuracy: 0.9939 - val_loss: 4.7866 - val_accuracy: 0.4167\n",
            "Epoch 75/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0091 - accuracy: 0.9933 - val_loss: 4.9396 - val_accuracy: 0.4583\n",
            "Epoch 76/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0085 - accuracy: 0.9942 - val_loss: 5.4318 - val_accuracy: 0.4583\n",
            "Epoch 77/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0085 - accuracy: 0.9939 - val_loss: 5.6555 - val_accuracy: 0.4167\n",
            "Epoch 78/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0481 - accuracy: 0.9861 - val_loss: 5.5768 - val_accuracy: 0.4583\n",
            "Epoch 79/200\n",
            "413/413 [==============================] - 30s 71ms/step - loss: 0.0608 - accuracy: 0.9806 - val_loss: 6.6442 - val_accuracy: 0.4583\n",
            "Epoch 80/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0440 - accuracy: 0.9852 - val_loss: 4.2841 - val_accuracy: 0.5000\n",
            "Epoch 81/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0186 - accuracy: 0.9906 - val_loss: 5.8878 - val_accuracy: 0.4583\n",
            "Epoch 82/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0364 - accuracy: 0.9852 - val_loss: 4.3897 - val_accuracy: 0.5417\n",
            "Epoch 83/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0240 - accuracy: 0.9894 - val_loss: 5.4811 - val_accuracy: 0.5000\n",
            "Epoch 84/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0109 - accuracy: 0.9939 - val_loss: 5.8983 - val_accuracy: 0.4583\n",
            "Epoch 85/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0085 - accuracy: 0.9933 - val_loss: 5.9111 - val_accuracy: 0.4583\n",
            "Epoch 86/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0085 - accuracy: 0.9930 - val_loss: 6.1505 - val_accuracy: 0.4583\n",
            "Epoch 87/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0081 - accuracy: 0.9936 - val_loss: 6.6807 - val_accuracy: 0.4167\n",
            "Epoch 88/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0082 - accuracy: 0.9939 - val_loss: 6.2352 - val_accuracy: 0.4167\n",
            "Epoch 89/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0182 - accuracy: 0.9936 - val_loss: 5.6528 - val_accuracy: 0.4167\n",
            "Epoch 90/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.1128 - accuracy: 0.9636 - val_loss: 5.3905 - val_accuracy: 0.4167\n",
            "Epoch 91/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0452 - accuracy: 0.9836 - val_loss: 5.0910 - val_accuracy: 0.5000\n",
            "Epoch 92/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0168 - accuracy: 0.9939 - val_loss: 5.0755 - val_accuracy: 0.4583\n",
            "Epoch 93/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0088 - accuracy: 0.9933 - val_loss: 6.0606 - val_accuracy: 0.4583\n",
            "Epoch 94/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0085 - accuracy: 0.9933 - val_loss: 6.3220 - val_accuracy: 0.4167\n",
            "Epoch 95/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0081 - accuracy: 0.9945 - val_loss: 6.7619 - val_accuracy: 0.4167\n",
            "Epoch 96/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0077 - accuracy: 0.9952 - val_loss: 6.6170 - val_accuracy: 0.4583\n",
            "Epoch 97/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0077 - accuracy: 0.9936 - val_loss: 6.7044 - val_accuracy: 0.5000\n",
            "Epoch 98/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0079 - accuracy: 0.9949 - val_loss: 6.5707 - val_accuracy: 0.4583\n",
            "Epoch 99/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0085 - accuracy: 0.9927 - val_loss: 7.2807 - val_accuracy: 0.4583\n",
            "Epoch 100/200\n",
            "413/413 [==============================] - 30s 71ms/step - loss: 0.1012 - accuracy: 0.9743 - val_loss: 3.2875 - val_accuracy: 0.4583\n",
            "Epoch 101/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0717 - accuracy: 0.9791 - val_loss: 3.9063 - val_accuracy: 0.4583\n",
            "Epoch 102/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0142 - accuracy: 0.9915 - val_loss: 4.7237 - val_accuracy: 0.5417\n",
            "Epoch 103/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0107 - accuracy: 0.9933 - val_loss: 4.3029 - val_accuracy: 0.4583\n",
            "Epoch 104/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0532 - accuracy: 0.9800 - val_loss: 3.6829 - val_accuracy: 0.5833\n",
            "Epoch 105/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0435 - accuracy: 0.9861 - val_loss: 4.6684 - val_accuracy: 0.4583\n",
            "Epoch 106/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0394 - accuracy: 0.9855 - val_loss: 3.9476 - val_accuracy: 0.4583\n",
            "Epoch 107/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0109 - accuracy: 0.9927 - val_loss: 4.5963 - val_accuracy: 0.5000\n",
            "Epoch 108/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0082 - accuracy: 0.9942 - val_loss: 5.1377 - val_accuracy: 0.5000\n",
            "Epoch 109/200\n",
            "413/413 [==============================] - 30s 71ms/step - loss: 0.0082 - accuracy: 0.9936 - val_loss: 4.9936 - val_accuracy: 0.5000\n",
            "Epoch 110/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0107 - accuracy: 0.9939 - val_loss: 4.0475 - val_accuracy: 0.4583\n",
            "Epoch 111/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0078 - accuracy: 0.9945 - val_loss: 5.0348 - val_accuracy: 0.5000\n",
            "Epoch 112/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0082 - accuracy: 0.9933 - val_loss: 4.8363 - val_accuracy: 0.4583\n",
            "Epoch 113/200\n",
            "413/413 [==============================] - 30s 71ms/step - loss: 0.0078 - accuracy: 0.9939 - val_loss: 5.1392 - val_accuracy: 0.4583\n",
            "Epoch 114/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0080 - accuracy: 0.9939 - val_loss: 4.9120 - val_accuracy: 0.4583\n",
            "Epoch 115/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0083 - accuracy: 0.9933 - val_loss: 5.1934 - val_accuracy: 0.4583\n",
            "Epoch 116/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0098 - accuracy: 0.9930 - val_loss: 6.0504 - val_accuracy: 0.5000\n",
            "Epoch 117/200\n",
            "413/413 [==============================] - 30s 71ms/step - loss: 0.1282 - accuracy: 0.9585 - val_loss: 4.5982 - val_accuracy: 0.4583\n",
            "Epoch 118/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0400 - accuracy: 0.9839 - val_loss: 6.3905 - val_accuracy: 0.5417\n",
            "Epoch 119/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0308 - accuracy: 0.9897 - val_loss: 5.1956 - val_accuracy: 0.5417\n",
            "Epoch 120/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0086 - accuracy: 0.9942 - val_loss: 5.5169 - val_accuracy: 0.5000\n",
            "Epoch 121/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0084 - accuracy: 0.9939 - val_loss: 5.3066 - val_accuracy: 0.5000\n",
            "Epoch 122/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0084 - accuracy: 0.9936 - val_loss: 5.6042 - val_accuracy: 0.5000\n",
            "Epoch 123/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0084 - accuracy: 0.9949 - val_loss: 5.5582 - val_accuracy: 0.5000\n",
            "Epoch 124/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.0082 - accuracy: 0.9942 - val_loss: 6.4326 - val_accuracy: 0.5417\n",
            "Epoch 125/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.1139 - accuracy: 0.9703 - val_loss: 4.7217 - val_accuracy: 0.5417\n",
            "Epoch 126/200\n",
            "413/413 [==============================] - 29s 69ms/step - loss: 0.0236 - accuracy: 0.9894 - val_loss: 3.9584 - val_accuracy: 0.5833\n",
            "Epoch 127/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0095 - accuracy: 0.9933 - val_loss: 4.7875 - val_accuracy: 0.5000\n",
            "Epoch 128/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0604 - accuracy: 0.9812 - val_loss: 4.3493 - val_accuracy: 0.5000\n",
            "Epoch 129/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0106 - accuracy: 0.9939 - val_loss: 4.7408 - val_accuracy: 0.5000\n",
            "Epoch 130/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0270 - accuracy: 0.9876 - val_loss: 3.7936 - val_accuracy: 0.5417\n",
            "Epoch 131/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0357 - accuracy: 0.9864 - val_loss: 4.0270 - val_accuracy: 0.5417\n",
            "Epoch 132/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0161 - accuracy: 0.9921 - val_loss: 3.7027 - val_accuracy: 0.5417\n",
            "Epoch 133/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0117 - accuracy: 0.9936 - val_loss: 4.7260 - val_accuracy: 0.4583\n",
            "Epoch 134/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0082 - accuracy: 0.9921 - val_loss: 5.5311 - val_accuracy: 0.4167\n",
            "Epoch 135/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0082 - accuracy: 0.9930 - val_loss: 5.3274 - val_accuracy: 0.4583\n",
            "Epoch 136/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0081 - accuracy: 0.9942 - val_loss: 5.4276 - val_accuracy: 0.4167\n",
            "Epoch 137/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0079 - accuracy: 0.9945 - val_loss: 5.3886 - val_accuracy: 0.5000\n",
            "Epoch 138/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0080 - accuracy: 0.9936 - val_loss: 5.6597 - val_accuracy: 0.5417\n",
            "Epoch 139/200\n",
            "413/413 [==============================] - 30s 71ms/step - loss: 0.0808 - accuracy: 0.9779 - val_loss: 4.9261 - val_accuracy: 0.4583\n",
            "Epoch 140/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0632 - accuracy: 0.9797 - val_loss: 4.5882 - val_accuracy: 0.5000\n",
            "Epoch 141/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0109 - accuracy: 0.9942 - val_loss: 4.8686 - val_accuracy: 0.5833\n",
            "Epoch 142/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0090 - accuracy: 0.9930 - val_loss: 6.0280 - val_accuracy: 0.5000\n",
            "Epoch 143/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0082 - accuracy: 0.9942 - val_loss: 6.0928 - val_accuracy: 0.5000\n",
            "Epoch 144/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0079 - accuracy: 0.9942 - val_loss: 6.2222 - val_accuracy: 0.5000\n",
            "Epoch 145/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0079 - accuracy: 0.9942 - val_loss: 6.5050 - val_accuracy: 0.5000\n",
            "Epoch 146/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0080 - accuracy: 0.9930 - val_loss: 6.9479 - val_accuracy: 0.4583\n",
            "Epoch 147/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0978 - accuracy: 0.9706 - val_loss: 5.7456 - val_accuracy: 0.4583\n",
            "Epoch 148/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0202 - accuracy: 0.9900 - val_loss: 5.0911 - val_accuracy: 0.4583\n",
            "Epoch 149/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0100 - accuracy: 0.9936 - val_loss: 5.3496 - val_accuracy: 0.4583\n",
            "Epoch 150/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0118 - accuracy: 0.9915 - val_loss: 6.2177 - val_accuracy: 0.4583\n",
            "Epoch 151/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0126 - accuracy: 0.9945 - val_loss: 5.7570 - val_accuracy: 0.4167\n",
            "Epoch 152/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0371 - accuracy: 0.9839 - val_loss: 4.8290 - val_accuracy: 0.4583\n",
            "Epoch 153/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0084 - accuracy: 0.9949 - val_loss: 5.3040 - val_accuracy: 0.4583\n",
            "Epoch 154/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0080 - accuracy: 0.9942 - val_loss: 5.7875 - val_accuracy: 0.4167\n",
            "Epoch 155/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0079 - accuracy: 0.9952 - val_loss: 5.8993 - val_accuracy: 0.4583\n",
            "Epoch 156/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0076 - accuracy: 0.9939 - val_loss: 6.1507 - val_accuracy: 0.4583\n",
            "Epoch 157/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0078 - accuracy: 0.9936 - val_loss: 6.3961 - val_accuracy: 0.5000\n",
            "Epoch 158/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0952 - accuracy: 0.9761 - val_loss: 3.4322 - val_accuracy: 0.5000\n",
            "Epoch 159/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0380 - accuracy: 0.9861 - val_loss: 4.4758 - val_accuracy: 0.4583\n",
            "Epoch 160/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0206 - accuracy: 0.9906 - val_loss: 6.3647 - val_accuracy: 0.4583\n",
            "Epoch 161/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0087 - accuracy: 0.9930 - val_loss: 5.1113 - val_accuracy: 0.5000\n",
            "Epoch 162/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0082 - accuracy: 0.9930 - val_loss: 5.6295 - val_accuracy: 0.5000\n",
            "Epoch 163/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0082 - accuracy: 0.9952 - val_loss: 5.9089 - val_accuracy: 0.4583\n",
            "Epoch 164/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0077 - accuracy: 0.9955 - val_loss: 5.2772 - val_accuracy: 0.5000\n",
            "Epoch 165/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0083 - accuracy: 0.9930 - val_loss: 5.8171 - val_accuracy: 0.4583\n",
            "Epoch 166/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0076 - accuracy: 0.9958 - val_loss: 5.6180 - val_accuracy: 0.4583\n",
            "Epoch 167/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0085 - accuracy: 0.9949 - val_loss: 6.7094 - val_accuracy: 0.5000\n",
            "Epoch 168/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0083 - accuracy: 0.9945 - val_loss: 5.7639 - val_accuracy: 0.5000\n",
            "Epoch 169/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0082 - accuracy: 0.9924 - val_loss: 4.9083 - val_accuracy: 0.5000\n",
            "Epoch 170/200\n",
            "413/413 [==============================] - 30s 71ms/step - loss: 0.0079 - accuracy: 0.9933 - val_loss: 5.7315 - val_accuracy: 0.5000\n",
            "Epoch 171/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0083 - accuracy: 0.9927 - val_loss: 5.7257 - val_accuracy: 0.5417\n",
            "Epoch 172/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0079 - accuracy: 0.9945 - val_loss: 5.6677 - val_accuracy: 0.5417\n",
            "Epoch 173/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0083 - accuracy: 0.9927 - val_loss: 7.5121 - val_accuracy: 0.5000\n",
            "Epoch 174/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0085 - accuracy: 0.9942 - val_loss: 8.1355 - val_accuracy: 0.5000\n",
            "Epoch 175/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.1587 - accuracy: 0.9630 - val_loss: 2.9751 - val_accuracy: 0.5417\n",
            "Epoch 176/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0512 - accuracy: 0.9849 - val_loss: 3.4475 - val_accuracy: 0.5000\n",
            "Epoch 177/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0108 - accuracy: 0.9918 - val_loss: 3.7713 - val_accuracy: 0.5000\n",
            "Epoch 178/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0083 - accuracy: 0.9942 - val_loss: 4.1371 - val_accuracy: 0.5417\n",
            "Epoch 179/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0076 - accuracy: 0.9967 - val_loss: 4.7434 - val_accuracy: 0.5000\n",
            "Epoch 180/200\n",
            "413/413 [==============================] - 29s 70ms/step - loss: 0.0081 - accuracy: 0.9933 - val_loss: 4.7846 - val_accuracy: 0.5417\n",
            "Epoch 181/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0077 - accuracy: 0.9949 - val_loss: 5.0002 - val_accuracy: 0.5417\n",
            "Epoch 182/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0081 - accuracy: 0.9942 - val_loss: 5.1283 - val_accuracy: 0.5000\n",
            "Epoch 183/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0108 - accuracy: 0.9924 - val_loss: 5.0642 - val_accuracy: 0.5000\n",
            "Epoch 184/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0567 - accuracy: 0.9821 - val_loss: 4.9412 - val_accuracy: 0.4583\n",
            "Epoch 185/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0627 - accuracy: 0.9839 - val_loss: 5.7712 - val_accuracy: 0.4583\n",
            "Epoch 186/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0359 - accuracy: 0.9879 - val_loss: 4.3554 - val_accuracy: 0.4583\n",
            "Epoch 187/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0343 - accuracy: 0.9882 - val_loss: 6.4787 - val_accuracy: 0.4583\n",
            "Epoch 188/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0100 - accuracy: 0.9936 - val_loss: 6.9986 - val_accuracy: 0.3750\n",
            "Epoch 189/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0173 - accuracy: 0.9924 - val_loss: 6.7468 - val_accuracy: 0.4167\n",
            "Epoch 190/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0079 - accuracy: 0.9955 - val_loss: 6.4401 - val_accuracy: 0.4583\n",
            "Epoch 191/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0080 - accuracy: 0.9939 - val_loss: 6.9422 - val_accuracy: 0.4583\n",
            "Epoch 192/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0081 - accuracy: 0.9952 - val_loss: 6.6579 - val_accuracy: 0.4583\n",
            "Epoch 193/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0084 - accuracy: 0.9949 - val_loss: 6.8810 - val_accuracy: 0.4167\n",
            "Epoch 194/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0079 - accuracy: 0.9955 - val_loss: 7.2081 - val_accuracy: 0.5000\n",
            "Epoch 195/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0079 - accuracy: 0.9936 - val_loss: 7.3787 - val_accuracy: 0.5000\n",
            "Epoch 196/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0077 - accuracy: 0.9945 - val_loss: 7.5944 - val_accuracy: 0.4583\n",
            "Epoch 197/200\n",
            "413/413 [==============================] - 30s 72ms/step - loss: 0.0078 - accuracy: 0.9936 - val_loss: 7.9250 - val_accuracy: 0.5000\n",
            "Epoch 198/200\n",
            "413/413 [==============================] - 30s 71ms/step - loss: 0.0077 - accuracy: 0.9952 - val_loss: 7.7822 - val_accuracy: 0.4583\n",
            "Epoch 199/200\n",
            "413/413 [==============================] - 29s 71ms/step - loss: 0.0079 - accuracy: 0.9955 - val_loss: 7.7561 - val_accuracy: 0.5000\n",
            "Epoch 200/200\n",
            "413/413 [==============================] - 30s 71ms/step - loss: 0.0077 - accuracy: 0.9942 - val_loss: 7.5313 - val_accuracy: 0.4583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj41CXepgq1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6cacb9-9801-4417-aa65-e191d0be7b01"
      },
      "source": [
        "print(\"[INFO] evaluating network...\")\n",
        "testGen.reset()\n",
        "predIdxs = model.predict_generator(testGen,\n",
        "\tsteps=(totalTest // config.BS) + 1)\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testGen.classes, predIdxs,\n",
        "\ttarget_names=testGen.class_indices.keys()))\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving model...\")\n",
        "model.save(config.MODEL_PATH, save_format=\"h5\")\n",
        "N = config.NUM_EPOCHS"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating network...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2035: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fire       0.78      0.78      0.78       200\n",
            "       human       0.55      0.64      0.59        42\n",
            "       infra       0.41      0.69      0.51       210\n",
            "        land       0.53      0.44      0.48       148\n",
            "    nodamage       0.49      0.11      0.18       211\n",
            "       water       0.59      0.70      0.64       211\n",
            "\n",
            "    accuracy                           0.55      1022\n",
            "   macro avg       0.56      0.56      0.53      1022\n",
            "weighted avg       0.56      0.55      0.52      1022\n",
            "\n",
            "[INFO] saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUGR_HTRgNhC"
      },
      "source": [
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(\"densenet disaster_Adam_plot.png\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEcaVZFjhvEF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d5ecd7-42e4-4412-db29-c85de1fa67d5"
      },
      "source": [
        "H.history"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': [0.46803998947143555,\n",
              "  0.6246591806411743,\n",
              "  0.7037261724472046,\n",
              "  0.7600727081298828,\n",
              "  0.7964253425598145,\n",
              "  0.8261132836341858,\n",
              "  0.8685246706008911,\n",
              "  0.8857921957969666,\n",
              "  0.9215389490127563,\n",
              "  0.9294153451919556,\n",
              "  0.9382005333900452,\n",
              "  0.9578915238380432,\n",
              "  0.9478945732116699,\n",
              "  0.9663738012313843,\n",
              "  0.9418358206748962,\n",
              "  0.9615268111228943,\n",
              "  0.9794001579284668,\n",
              "  0.9709178805351257,\n",
              "  0.9651620984077454,\n",
              "  0.9812178015708923,\n",
              "  0.9742501974105835,\n",
              "  0.9797031283378601,\n",
              "  0.9642532467842102,\n",
              "  0.9818236827850342,\n",
              "  0.9839442372322083,\n",
              "  0.9660708904266357,\n",
              "  0.9869736433029175,\n",
              "  0.9806119203567505,\n",
              "  0.9869736433029175,\n",
              "  0.971220850944519,\n",
              "  0.9890941977500916,\n",
              "  0.9706149697303772,\n",
              "  0.9772796034812927,\n",
              "  0.9875795245170593,\n",
              "  0.9890941977500916,\n",
              "  0.9806119203567505,\n",
              "  0.9724326133728027,\n",
              "  0.9827325344085693,\n",
              "  0.988791286945343,\n",
              "  0.9781884551048279,\n",
              "  0.9851559996604919,\n",
              "  0.9806119203567505,\n",
              "  0.9915177226066589,\n",
              "  0.9933353662490845,\n",
              "  0.9890941977500916,\n",
              "  0.9748560786247253,\n",
              "  0.977582573890686,\n",
              "  0.9942441582679749,\n",
              "  0.9939412474632263,\n",
              "  0.9921236038208008,\n",
              "  0.9942441582679749,\n",
              "  0.9933353662490845,\n",
              "  0.9772796034812927,\n",
              "  0.962738573551178,\n",
              "  0.9863677620887756,\n",
              "  0.9897000789642334,\n",
              "  0.9884883165359497,\n",
              "  0.9769766926765442,\n",
              "  0.9893971681594849,\n",
              "  0.9939412474632263,\n",
              "  0.9915177226066589,\n",
              "  0.9875795245170593,\n",
              "  0.9906089305877686,\n",
              "  0.9815207719802856,\n",
              "  0.9845501184463501,\n",
              "  0.9903059601783752,\n",
              "  0.993638277053833,\n",
              "  0.9927294850349426,\n",
              "  0.9945471286773682,\n",
              "  0.9930323958396912,\n",
              "  0.9687973260879517,\n",
              "  0.9839442372322083,\n",
              "  0.9851559996604919,\n",
              "  0.9939412474632263,\n",
              "  0.9933353662490845,\n",
              "  0.9942441582679749,\n",
              "  0.9939412474632263,\n",
              "  0.9860648512840271,\n",
              "  0.9806119203567505,\n",
              "  0.9851559996604919,\n",
              "  0.9906089305877686,\n",
              "  0.9851559996604919,\n",
              "  0.9893971681594849,\n",
              "  0.9939412474632263,\n",
              "  0.9933353662490845,\n",
              "  0.9930323958396912,\n",
              "  0.993638277053833,\n",
              "  0.9939412474632263,\n",
              "  0.993638277053833,\n",
              "  0.9636473655700684,\n",
              "  0.9836413264274597,\n",
              "  0.9939412474632263,\n",
              "  0.9933353662490845,\n",
              "  0.9933353662490845,\n",
              "  0.9945471286773682,\n",
              "  0.99515300989151,\n",
              "  0.993638277053833,\n",
              "  0.9948500394821167,\n",
              "  0.9927294850349426,\n",
              "  0.9742501974105835,\n",
              "  0.9790972471237183,\n",
              "  0.9915177226066589,\n",
              "  0.9933353662490845,\n",
              "  0.9800060391426086,\n",
              "  0.9860648512840271,\n",
              "  0.9854589700698853,\n",
              "  0.9927294850349426,\n",
              "  0.9942441582679749,\n",
              "  0.993638277053833,\n",
              "  0.9939412474632263,\n",
              "  0.9945471286773682,\n",
              "  0.9933353662490845,\n",
              "  0.9939412474632263,\n",
              "  0.9939412474632263,\n",
              "  0.9933353662490845,\n",
              "  0.9930323958396912,\n",
              "  0.9584974050521851,\n",
              "  0.9839442372322083,\n",
              "  0.9897000789642334,\n",
              "  0.9942441582679749,\n",
              "  0.9939412474632263,\n",
              "  0.993638277053833,\n",
              "  0.9948500394821167,\n",
              "  0.9942441582679749,\n",
              "  0.9703119993209839,\n",
              "  0.9893971681594849,\n",
              "  0.9933353662490845,\n",
              "  0.9812178015708923,\n",
              "  0.9939412474632263,\n",
              "  0.9875795245170593,\n",
              "  0.9863677620887756,\n",
              "  0.9921236038208008,\n",
              "  0.993638277053833,\n",
              "  0.9921236038208008,\n",
              "  0.9930323958396912,\n",
              "  0.9942441582679749,\n",
              "  0.9945471286773682,\n",
              "  0.993638277053833,\n",
              "  0.9778854846954346,\n",
              "  0.9797031283378601,\n",
              "  0.9942441582679749,\n",
              "  0.9930323958396912,\n",
              "  0.9942441582679749,\n",
              "  0.9942441582679749,\n",
              "  0.9942441582679749,\n",
              "  0.9930323958396912,\n",
              "  0.9706149697303772,\n",
              "  0.9900030493736267,\n",
              "  0.993638277053833,\n",
              "  0.9915177226066589,\n",
              "  0.9945471286773682,\n",
              "  0.9839442372322083,\n",
              "  0.9948500394821167,\n",
              "  0.9942441582679749,\n",
              "  0.99515300989151,\n",
              "  0.9939412474632263,\n",
              "  0.993638277053833,\n",
              "  0.976067841053009,\n",
              "  0.9860648512840271,\n",
              "  0.9906089305877686,\n",
              "  0.9930323958396912,\n",
              "  0.9930323958396912,\n",
              "  0.99515300989151,\n",
              "  0.9954559206962585,\n",
              "  0.9930323958396912,\n",
              "  0.9957588315010071,\n",
              "  0.9948500394821167,\n",
              "  0.9945471286773682,\n",
              "  0.9924265146255493,\n",
              "  0.9933353662490845,\n",
              "  0.9927294850349426,\n",
              "  0.9945471286773682,\n",
              "  0.9927294850349426,\n",
              "  0.9942441582679749,\n",
              "  0.9630414843559265,\n",
              "  0.9848530888557434,\n",
              "  0.9918206334114075,\n",
              "  0.9942441582679749,\n",
              "  0.9966676831245422,\n",
              "  0.9933353662490845,\n",
              "  0.9948500394821167,\n",
              "  0.9942441582679749,\n",
              "  0.9924265146255493,\n",
              "  0.9821266531944275,\n",
              "  0.9839442372322083,\n",
              "  0.9878824353218079,\n",
              "  0.9881854057312012,\n",
              "  0.993638277053833,\n",
              "  0.9924265146255493,\n",
              "  0.9954559206962585,\n",
              "  0.9939412474632263,\n",
              "  0.99515300989151,\n",
              "  0.9948500394821167,\n",
              "  0.9954559206962585,\n",
              "  0.993638277053833,\n",
              "  0.9945471286773682,\n",
              "  0.993638277053833,\n",
              "  0.99515300989151,\n",
              "  0.9954559206962585,\n",
              "  0.9942441582679749],\n",
              " 'loss': [1.4656715393066406,\n",
              "  1.0135698318481445,\n",
              "  0.8175378441810608,\n",
              "  0.6779369711875916,\n",
              "  0.5633544921875,\n",
              "  0.4724474549293518,\n",
              "  0.3758833706378937,\n",
              "  0.32386067509651184,\n",
              "  0.2290835827589035,\n",
              "  0.20355598628520966,\n",
              "  0.17793668806552887,\n",
              "  0.1290004998445511,\n",
              "  0.15427561104297638,\n",
              "  0.09272221475839615,\n",
              "  0.18702538311481476,\n",
              "  0.10892181098461151,\n",
              "  0.06580327451229095,\n",
              "  0.09638465940952301,\n",
              "  0.10656754672527313,\n",
              "  0.06287537515163422,\n",
              "  0.071895070374012,\n",
              "  0.050439294427633286,\n",
              "  0.1221502274274826,\n",
              "  0.05049807205796242,\n",
              "  0.04108242318034172,\n",
              "  0.11703108251094818,\n",
              "  0.038819000124931335,\n",
              "  0.05359265208244324,\n",
              "  0.028724879026412964,\n",
              "  0.08776029944419861,\n",
              "  0.0347248800098896,\n",
              "  0.09284155070781708,\n",
              "  0.06952358782291412,\n",
              "  0.03258475661277771,\n",
              "  0.023858539760112762,\n",
              "  0.060903675854206085,\n",
              "  0.08134616911411285,\n",
              "  0.04632088541984558,\n",
              "  0.023823412135243416,\n",
              "  0.06308827549219131,\n",
              "  0.038642268627882004,\n",
              "  0.07105866819620132,\n",
              "  0.02151820994913578,\n",
              "  0.011455568484961987,\n",
              "  0.02425442822277546,\n",
              "  0.08683235198259354,\n",
              "  0.06337596476078033,\n",
              "  0.010979757644236088,\n",
              "  0.011040770448744297,\n",
              "  0.010858423076570034,\n",
              "  0.009501175954937935,\n",
              "  0.009835492819547653,\n",
              "  0.05726468563079834,\n",
              "  0.12331178784370422,\n",
              "  0.03925727680325508,\n",
              "  0.021702269092202187,\n",
              "  0.027109067887067795,\n",
              "  0.06503644585609436,\n",
              "  0.024571850895881653,\n",
              "  0.012916446663439274,\n",
              "  0.017804579809308052,\n",
              "  0.03904002159833908,\n",
              "  0.0187523290514946,\n",
              "  0.05586561560630798,\n",
              "  0.04408085346221924,\n",
              "  0.018959037959575653,\n",
              "  0.008838680572807789,\n",
              "  0.009272439405322075,\n",
              "  0.008806677535176277,\n",
              "  0.012976795434951782,\n",
              "  0.10730905085802078,\n",
              "  0.05885676294565201,\n",
              "  0.046291109174489975,\n",
              "  0.010302355512976646,\n",
              "  0.00906872283667326,\n",
              "  0.008548800833523273,\n",
              "  0.008501962758600712,\n",
              "  0.04814447462558746,\n",
              "  0.06082108989357948,\n",
              "  0.0440177284181118,\n",
              "  0.018592707812786102,\n",
              "  0.03641727566719055,\n",
              "  0.02395704761147499,\n",
              "  0.010942415334284306,\n",
              "  0.008451497182250023,\n",
              "  0.00847654975950718,\n",
              "  0.00805574283003807,\n",
              "  0.008192893117666245,\n",
              "  0.018189582973718643,\n",
              "  0.11284378916025162,\n",
              "  0.045170173048973083,\n",
              "  0.01675180345773697,\n",
              "  0.008844750933349133,\n",
              "  0.008505376987159252,\n",
              "  0.008092674426734447,\n",
              "  0.0076542566530406475,\n",
              "  0.0076649426482617855,\n",
              "  0.00786052830517292,\n",
              "  0.008515359833836555,\n",
              "  0.10116761922836304,\n",
              "  0.07174021005630493,\n",
              "  0.014228752814233303,\n",
              "  0.010722578503191471,\n",
              "  0.05318831279873848,\n",
              "  0.043502770364284515,\n",
              "  0.039359889924526215,\n",
              "  0.010886300355196,\n",
              "  0.008244375698268414,\n",
              "  0.00819467380642891,\n",
              "  0.010723927989602089,\n",
              "  0.007824385538697243,\n",
              "  0.0081595778465271,\n",
              "  0.0078072198666632175,\n",
              "  0.00795625802129507,\n",
              "  0.008260367438197136,\n",
              "  0.009812894277274609,\n",
              "  0.1282167136669159,\n",
              "  0.03996000438928604,\n",
              "  0.030820176005363464,\n",
              "  0.008648810908198357,\n",
              "  0.008420602418482304,\n",
              "  0.008395405486226082,\n",
              "  0.008360080420970917,\n",
              "  0.008161150850355625,\n",
              "  0.1138777807354927,\n",
              "  0.023623449727892876,\n",
              "  0.009511115960776806,\n",
              "  0.06035645306110382,\n",
              "  0.010641767643392086,\n",
              "  0.026957599446177483,\n",
              "  0.035731762647628784,\n",
              "  0.016087491065263748,\n",
              "  0.011680284515023232,\n",
              "  0.00817027222365141,\n",
              "  0.008186561986804008,\n",
              "  0.008066815324127674,\n",
              "  0.007916462607681751,\n",
              "  0.007954751141369343,\n",
              "  0.08079975843429565,\n",
              "  0.06320210546255112,\n",
              "  0.010946325957775116,\n",
              "  0.009033760987222195,\n",
              "  0.008217407390475273,\n",
              "  0.007883231155574322,\n",
              "  0.007854551076889038,\n",
              "  0.007962845265865326,\n",
              "  0.09777345508337021,\n",
              "  0.020190292969346046,\n",
              "  0.010031626559793949,\n",
              "  0.011761598289012909,\n",
              "  0.012610639445483685,\n",
              "  0.03705983981490135,\n",
              "  0.008415437303483486,\n",
              "  0.008046980947256088,\n",
              "  0.007861332036554813,\n",
              "  0.007568309083580971,\n",
              "  0.00783343892544508,\n",
              "  0.0952065959572792,\n",
              "  0.03804029896855354,\n",
              "  0.020646078512072563,\n",
              "  0.008723024278879166,\n",
              "  0.008236775174736977,\n",
              "  0.008161849342286587,\n",
              "  0.007688973564654589,\n",
              "  0.008277124725282192,\n",
              "  0.007597639225423336,\n",
              "  0.008548330515623093,\n",
              "  0.008312609978020191,\n",
              "  0.008183939382433891,\n",
              "  0.007920106872916222,\n",
              "  0.008314277976751328,\n",
              "  0.007858692668378353,\n",
              "  0.008311310783028603,\n",
              "  0.008515470661222935,\n",
              "  0.15867936611175537,\n",
              "  0.05119761452078819,\n",
              "  0.010842761024832726,\n",
              "  0.008291708305478096,\n",
              "  0.007609046529978514,\n",
              "  0.008104097098112106,\n",
              "  0.007703099399805069,\n",
              "  0.008082454092800617,\n",
              "  0.010750914923846722,\n",
              "  0.05667169764637947,\n",
              "  0.06272196024656296,\n",
              "  0.03589050844311714,\n",
              "  0.03426491096615791,\n",
              "  0.010019891895353794,\n",
              "  0.017321739345788956,\n",
              "  0.007915456779301167,\n",
              "  0.00800447165966034,\n",
              "  0.008064117282629013,\n",
              "  0.008373372256755829,\n",
              "  0.007893435657024384,\n",
              "  0.007883955724537373,\n",
              "  0.007746211253106594,\n",
              "  0.00779362116008997,\n",
              "  0.0077233160845935345,\n",
              "  0.007927576079964638,\n",
              "  0.007722398266196251],\n",
              " 'val_accuracy': [0.3333333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.5833333134651184,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.5833333134651184,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.5416666865348816,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.625,\n",
              "  0.5,\n",
              "  0.5833333134651184,\n",
              "  0.4583333432674408,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.5416666865348816,\n",
              "  0.4583333432674408,\n",
              "  0.5416666865348816,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.5416666865348816,\n",
              "  0.4166666567325592,\n",
              "  0.5416666865348816,\n",
              "  0.375,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.5416666865348816,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.5416666865348816,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4166666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.5416666865348816,\n",
              "  0.4583333432674408,\n",
              "  0.5833333134651184,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.5,\n",
              "  0.5416666865348816,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.5833333134651184,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5416666865348816,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5416666865348816,\n",
              "  0.5,\n",
              "  0.5416666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.375,\n",
              "  0.4166666567325592,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4583333432674408,\n",
              "  0.4166666567325592,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.4583333432674408,\n",
              "  0.5,\n",
              "  0.4583333432674408],\n",
              " 'val_loss': [1.564584732055664,\n",
              "  1.5157002210617065,\n",
              "  1.6161231994628906,\n",
              "  1.840275764465332,\n",
              "  1.678942322731018,\n",
              "  1.7074130773544312,\n",
              "  1.6119909286499023,\n",
              "  2.0829832553863525,\n",
              "  2.259345293045044,\n",
              "  1.919573426246643,\n",
              "  1.8458229303359985,\n",
              "  2.4539260864257812,\n",
              "  2.3744943141937256,\n",
              "  3.6207644939422607,\n",
              "  2.292120933532715,\n",
              "  2.255310535430908,\n",
              "  3.8966615200042725,\n",
              "  3.882357358932495,\n",
              "  2.8538761138916016,\n",
              "  2.5124549865722656,\n",
              "  3.2892587184906006,\n",
              "  3.470010995864868,\n",
              "  2.7471399307250977,\n",
              "  3.3519370555877686,\n",
              "  3.5436513423919678,\n",
              "  2.8084919452667236,\n",
              "  4.059812545776367,\n",
              "  3.4683191776275635,\n",
              "  4.130900859832764,\n",
              "  2.8131141662597656,\n",
              "  3.8241217136383057,\n",
              "  3.6152431964874268,\n",
              "  2.9394333362579346,\n",
              "  3.7329156398773193,\n",
              "  3.667642831802368,\n",
              "  3.3027737140655518,\n",
              "  4.586256504058838,\n",
              "  4.4237589836120605,\n",
              "  3.677375555038452,\n",
              "  6.503111362457275,\n",
              "  3.9244396686553955,\n",
              "  4.2643818855285645,\n",
              "  4.299434185028076,\n",
              "  4.151564598083496,\n",
              "  6.248499393463135,\n",
              "  3.0937793254852295,\n",
              "  3.6149113178253174,\n",
              "  3.6185219287872314,\n",
              "  4.3101277351379395,\n",
              "  4.479005336761475,\n",
              "  4.6161112785339355,\n",
              "  5.27695894241333,\n",
              "  4.2414069175720215,\n",
              "  3.927658796310425,\n",
              "  4.206368923187256,\n",
              "  5.941612243652344,\n",
              "  5.772271633148193,\n",
              "  4.4939703941345215,\n",
              "  3.891070604324341,\n",
              "  5.858644485473633,\n",
              "  5.261273384094238,\n",
              "  3.456350564956665,\n",
              "  4.33566427230835,\n",
              "  5.308570861816406,\n",
              "  3.404430389404297,\n",
              "  4.923440456390381,\n",
              "  5.7233757972717285,\n",
              "  5.0919013023376465,\n",
              "  5.845255374908447,\n",
              "  4.911845684051514,\n",
              "  5.110800266265869,\n",
              "  3.4801437854766846,\n",
              "  4.835025310516357,\n",
              "  4.786597728729248,\n",
              "  4.939624309539795,\n",
              "  5.431786060333252,\n",
              "  5.655489444732666,\n",
              "  5.57675313949585,\n",
              "  6.644245147705078,\n",
              "  4.284095764160156,\n",
              "  5.887828350067139,\n",
              "  4.389729976654053,\n",
              "  5.481113910675049,\n",
              "  5.898321628570557,\n",
              "  5.911056995391846,\n",
              "  6.150460720062256,\n",
              "  6.6807475090026855,\n",
              "  6.235185146331787,\n",
              "  5.652787685394287,\n",
              "  5.390455722808838,\n",
              "  5.090970516204834,\n",
              "  5.075479507446289,\n",
              "  6.060577392578125,\n",
              "  6.321950435638428,\n",
              "  6.761871337890625,\n",
              "  6.616998195648193,\n",
              "  6.704381942749023,\n",
              "  6.570676326751709,\n",
              "  7.280698299407959,\n",
              "  3.287450075149536,\n",
              "  3.9062764644622803,\n",
              "  4.723705768585205,\n",
              "  4.302947521209717,\n",
              "  3.682917594909668,\n",
              "  4.66844367980957,\n",
              "  3.947582960128784,\n",
              "  4.596280574798584,\n",
              "  5.137686252593994,\n",
              "  4.993556499481201,\n",
              "  4.04745626449585,\n",
              "  5.034839630126953,\n",
              "  4.83629035949707,\n",
              "  5.1392364501953125,\n",
              "  4.9119791984558105,\n",
              "  5.1933913230896,\n",
              "  6.050437927246094,\n",
              "  4.598199844360352,\n",
              "  6.390462398529053,\n",
              "  5.19559907913208,\n",
              "  5.516932010650635,\n",
              "  5.306613922119141,\n",
              "  5.604151248931885,\n",
              "  5.558172225952148,\n",
              "  6.4326324462890625,\n",
              "  4.721696376800537,\n",
              "  3.958449602127075,\n",
              "  4.787485122680664,\n",
              "  4.349256992340088,\n",
              "  4.740843296051025,\n",
              "  3.7936031818389893,\n",
              "  4.0269622802734375,\n",
              "  3.702676773071289,\n",
              "  4.726021766662598,\n",
              "  5.531147480010986,\n",
              "  5.327350616455078,\n",
              "  5.427553653717041,\n",
              "  5.388553142547607,\n",
              "  5.659722805023193,\n",
              "  4.926061153411865,\n",
              "  4.588173866271973,\n",
              "  4.86859130859375,\n",
              "  6.0280327796936035,\n",
              "  6.092782974243164,\n",
              "  6.222235202789307,\n",
              "  6.505038738250732,\n",
              "  6.947892665863037,\n",
              "  5.7456207275390625,\n",
              "  5.091125965118408,\n",
              "  5.3496222496032715,\n",
              "  6.217657566070557,\n",
              "  5.75697660446167,\n",
              "  4.828985214233398,\n",
              "  5.304024696350098,\n",
              "  5.787452220916748,\n",
              "  5.899294376373291,\n",
              "  6.150679111480713,\n",
              "  6.396068572998047,\n",
              "  3.43221378326416,\n",
              "  4.475810527801514,\n",
              "  6.364687442779541,\n",
              "  5.1113057136535645,\n",
              "  5.629544734954834,\n",
              "  5.908934116363525,\n",
              "  5.277237415313721,\n",
              "  5.817067623138428,\n",
              "  5.618028163909912,\n",
              "  6.70944356918335,\n",
              "  5.763925075531006,\n",
              "  4.908339023590088,\n",
              "  5.731481075286865,\n",
              "  5.725656986236572,\n",
              "  5.667704105377197,\n",
              "  7.512124538421631,\n",
              "  8.135472297668457,\n",
              "  2.975079298019409,\n",
              "  3.4475128650665283,\n",
              "  3.7713329792022705,\n",
              "  4.137123107910156,\n",
              "  4.74338960647583,\n",
              "  4.784641742706299,\n",
              "  5.000234127044678,\n",
              "  5.128332614898682,\n",
              "  5.064176559448242,\n",
              "  4.941193103790283,\n",
              "  5.771176815032959,\n",
              "  4.35540246963501,\n",
              "  6.47873067855835,\n",
              "  6.99859619140625,\n",
              "  6.746788501739502,\n",
              "  6.440097808837891,\n",
              "  6.942245006561279,\n",
              "  6.657928466796875,\n",
              "  6.881008148193359,\n",
              "  7.208139896392822,\n",
              "  7.378661632537842,\n",
              "  7.594446182250977,\n",
              "  7.925047397613525,\n",
              "  7.782227993011475,\n",
              "  7.756139755249023,\n",
              "  7.53125524520874]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyf3CS9ZiHje"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}